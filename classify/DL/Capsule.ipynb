{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current directory\n",
    "import os\n",
    "os.chdir('F:\\Work\\Experiment\\pLM4ACE\\model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_new = pd.read_csv(r\"fusion_features\\Data\\features_select\\PCA_All.csv\", index_col=False, header=None)\n",
    "y_new = pd.read_csv(\"fusion_features\\Data\\label.csv\", index_col=False, header=None)\n",
    "\n",
    "print(X_new.shape)\n",
    "print(y_new.shape)\n",
    "print(np.count_nonzero(y_new==0))\n",
    "print(np.count_nonzero(y_new==1))\n",
    "\n",
    "X_new = np.array(X_new, 'float32')\n",
    "y_new = np.array(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10折交叉验证\n",
    "import os,sys,math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import scale\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Lambda, Concatenate, Multiply\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import adam_v2 #Adam 改为 adam_v2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_whole, X_ind_test, y_train_whole, y_ind_test = train_test_split(X_new, y_new, test_size=0.2, random_state=1111)\n",
    "\n",
    "\n",
    "# train_set = scale(X_train_whole)\n",
    "[sample_num, input_dimwx]=np.shape(X_train_whole)\n",
    "X = np.reshape(X_train_whole, (sample_num, 1, input_dimwx, 1))\n",
    "y = y_train_whole\n",
    "\n",
    "\n",
    "def squash(vectors, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "def Capsule_Layer():\n",
    "    img = Input(shape=(1,input_dimwx,1))\n",
    "    x = Conv2D(filters=64, kernel_size=(1,9), strides=2, padding='valid', name='conv1')(img)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(1,9), strides=2, padding='valid', name='conv1')(img)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    \"\"\"\n",
    "    NOTE: Capsule architecture starts from here.\n",
    "    \"\"\"\n",
    "    ##### primarycaps coming first ##### \n",
    "    x = Conv2D(filters=32, kernel_size=(1,3), strides=2, padding='valid', name='primarycap_conv2')(x)    \n",
    "    [aa,bb,cc,dd] = x.shape\n",
    "    numx = int(cc)\n",
    "    x = Reshape(target_shape=[-1, numx], name='primarycap_reshape')(x)\n",
    "    x = Lambda(squash, name='primarycap_squash')(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    ##### digitcaps are here ##### \n",
    "    x = Flatten()(x)\n",
    "    uhat = Dense(32, kernel_initializer='he_normal', bias_initializer='zeros', name='uhat_digitcaps')(x)\n",
    "    c = Activation('softmax', name='softmax_digitcaps1')(uhat) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(32)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    \"\"\"\n",
    "    NOTE: Squashing the capsule outputs creates severe blurry artifacts, thus we replace it with Leaky ReLu.\n",
    "    \"\"\"\n",
    "    s_j = LeakyReLU()(x)\n",
    "    ##### we will repeat the routing part 2 more times (num_routing=3) to unfold the loop\n",
    "    c = Activation('softmax', name='softmax_digitcaps2')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(32)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "\n",
    "    c = Activation('softmax', name='softmax_digitcaps3')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(32)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "    \n",
    "    c = Activation('softmax', name='softmax_digitcaps4')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(32)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "\n",
    "    pred = Dense(2, activation='sigmoid')(s_j)\n",
    "    model = Model (img, pred)\n",
    "    # model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam (0.0002, 0.5), metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "AUC_collecton = []\n",
    "AP=[]\n",
    "\n",
    "mean_recall = np.linspace(0, 1, 100)\n",
    "all_precision = []\n",
    "\n",
    "base_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = 0.0\n",
    "# 新的TPR集合\n",
    "interp_tpr_collection = []\n",
    "\n",
    "\n",
    "def categorical_probas_to_classes(p):\n",
    "    return np.argmax(p, axis=1)\n",
    "\n",
    "\n",
    "def to_categorical(y, nb_classes=None):\n",
    "    y = np.array(y, dtype='int')\n",
    "    if not nb_classes:\n",
    "        nb_classes = np.max(y)+1\n",
    "    Y = np.zeros((len(y), nb_classes))\n",
    "    for i in range(len(y)):\n",
    "        Y[i, y[i]] = 1\n",
    "    return Y\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train, test in skf.split(X, y):\n",
    "    X_train, X_valid, y_train, y_valid = np.take(X, train.tolist(), axis=0), np.take(X, test.tolist(), axis=0), np.take(y, train.tolist(), axis=0), np.take(y, test.tolist(), axis=0)\n",
    "    y_train = to_categorical(y_train)\n",
    "    cv_clf = Capsule_Layer()\n",
    "    hist = cv_clf.fit(X_train, y_train, batch_size=64, epochs=60)\n",
    "    y_score = cv_clf.predict(X_valid)\n",
    "    y_class = categorical_probas_to_classes(y_score)\n",
    "    TP, FP, FN, TN = confusion_matrix(y_valid, y_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
    "    Sn_collecton.append(TP/(TP+FN))\n",
    "    Sp_collecton.append(TN/(TN+FP))\n",
    "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "    MCC_collecton.append(MCC)\n",
    "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_valid, y_score[:, 1])\n",
    "    interp_tpr = np.interp(base_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    interp_tpr_collection.append(interp_tpr)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "    AUC_collecton.append(auc_roc)\n",
    "    # PR curve\n",
    "    precision, recall, _ = precision_recall_curve(y_valid, y_score[:, 1])\n",
    "    average_precision = average_precision_score(y_valid, y_score[:, 1])\n",
    "    recall = np.flipud(recall)\n",
    "    precision = np.flipud(precision)\n",
    "\n",
    "    mean_precision = np.interp(mean_recall, recall, precision)\n",
    "    all_precision.append(mean_precision)\n",
    "    AP.append(average_precision)\n",
    "\n",
    "# 输出结果\n",
    "print(round(statistics.mean(BACC_collecton),3),'±',round(statistics.stdev(BACC_collecton),3))\n",
    "print(round(statistics.mean(Sn_collecton),3),'±',round(statistics.stdev(Sn_collecton),3))\n",
    "print(round(statistics.mean(Sp_collecton),3),'±',round(statistics.stdev(Sp_collecton),3))\n",
    "print(round(statistics.mean(MCC_collecton),3),'±',round(statistics.stdev(MCC_collecton),3))\n",
    "print(round(statistics.mean(AUC_collecton),3),'±',round(statistics.stdev(AUC_collecton),3))\n",
    "print(round(statistics.mean(AP),3),'±',round(statistics.stdev(AP),3))\n",
    "\n",
    "# 在所有交叉验证循环结束后，计算TPR的均值\n",
    "mean_tpr = np.mean(interp_tpr_collection, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "\n",
    "# Calculate the mean precision\n",
    "mean_precision = np.mean(all_precision, axis=0)\n",
    "\n",
    "# 保存ROC曲线相关参数\n",
    "np.savez(r'Draw graphics\\ROC curve\\PCA_All\\Capsule_cross_vaild.npz', fpr=base_fpr, tpr=mean_tpr, roc_auc=AUC_collecton)\n",
    "\n",
    "# 保存PR曲线相关参数\n",
    "np.savez(r'Draw graphics\\PR curve\\PCA_All\\Capsule_cross_vaild.npz', recall=mean_recall, precision=mean_precision, average_precision=AP)\n",
    "\n",
    "# 绘制ROC曲线\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(base_fpr, mean_tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % np.mean(AUC_collecton))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('10 k-fold cross vaild')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 独立测试集\n",
    "import os,sys,math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import scale\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Lambda, Concatenate, Multiply\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import adam_v2 #Adam 改为 adam_v2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "# X_new = scale(X_new)\n",
    "[sample_num, input_dimwx]=np.shape(X_new)\n",
    "X = np.reshape(X_new, (sample_num, 1, input_dimwx, 1))\n",
    "y = y_new\n",
    "\n",
    "\n",
    "def squash(vectors, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "def Capsule_Layer():\n",
    "    img = Input(shape=(1,input_dimwx,1))\n",
    "    x = Conv2D(filters=64, kernel_size=(1,9), strides=2, padding='valid', name='conv1')(img)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(1,9), strides=2, padding='valid', name='conv1')(img)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    \"\"\"\n",
    "    NOTE: Capsule architecture starts from here.\n",
    "    \"\"\"\n",
    "    ##### primarycaps coming first ##### \n",
    "    x = Conv2D(filters=32, kernel_size=(1,3), strides=2, padding='valid', name='primarycap_conv2')(x)    \n",
    "    [aa,bb,cc,dd] = x.shape\n",
    "    numx = int(cc)\n",
    "    x = Reshape(target_shape=[-1, numx], name='primarycap_reshape')(x)\n",
    "    x = Lambda(squash, name='primarycap_squash')(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    ##### digitcaps are here ##### \n",
    "    x = Flatten()(x)\n",
    "    uhat = Dense(32, kernel_initializer='he_normal', bias_initializer='zeros', name='uhat_digitcaps')(x)\n",
    "    c = Activation('softmax', name='softmax_digitcaps1')(uhat) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(32)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    \"\"\"\n",
    "    NOTE: Squashing the capsule outputs creates severe blurry artifacts, thus we replace it with Leaky ReLu.\n",
    "    \"\"\"\n",
    "    s_j = LeakyReLU()(x)\n",
    "    ##### we will repeat the routing part 2 more times (num_routing=3) to unfold the loop\n",
    "    c = Activation('softmax', name='softmax_digitcaps2')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(32)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "\n",
    "    c = Activation('softmax', name='softmax_digitcaps3')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(32)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "    \n",
    "    c = Activation('softmax', name='softmax_digitcaps4')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(32)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "\n",
    "    pred = Dense(2, activation='sigmoid')(s_j)\n",
    "    model = Model (img, pred)\n",
    "    # model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam (0.0002, 0.5), metrics=['binary_accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "AUC_collecton = []\n",
    "AP=[]\n",
    "mean_recall = np.linspace(0, 1, 100)\n",
    "all_precision = []\n",
    "\n",
    "base_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = 0.0\n",
    "# 新的TPR集合\n",
    "interp_tpr_collection = []\n",
    "\n",
    "\n",
    "def categorical_probas_to_classes(p):\n",
    "    return np.argmax(p, axis=1)\n",
    "\n",
    "\n",
    "def to_categorical(y, nb_classes=None):\n",
    "    y = np.array(y, dtype='int')\n",
    "    if not nb_classes:\n",
    "        nb_classes = np.max(y)+1\n",
    "    Y = np.zeros((len(y), nb_classes))\n",
    "    for i in range(len(y)):\n",
    "        Y[i, y[i]] = 1\n",
    "    return Y\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    # dataset splitting\n",
    "    X_train_whole, X_ind_test, y_train_whole, y_ind_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    y_train_whole = to_categorical(y_train_whole)\n",
    "    cv_clf = Capsule_Layer()\n",
    "    hist = cv_clf.fit(X_train_whole, y_train_whole, batch_size=64, epochs=60)\n",
    "    y_score = cv_clf.predict(X_ind_test)\n",
    "    y_class = categorical_probas_to_classes(y_score)\n",
    "    TP, FP, FN, TN = confusion_matrix(y_ind_test, y_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
    "    Sn_collecton.append(TP/(TP+FN))\n",
    "    Sp_collecton.append(TN/(TN+FP))\n",
    "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "    MCC_collecton.append(MCC)\n",
    "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_ind_test, y_score[:, 1])\n",
    "    interp_tpr = np.interp(base_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    interp_tpr_collection.append(interp_tpr)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "    AUC_collecton.append(auc_roc)\n",
    "    # PR curve\n",
    "    precision, recall, _ = precision_recall_curve(y_ind_test, y_score[:, 1])\n",
    "    average_precision = average_precision_score(y_ind_test, y_score[:, 1])\n",
    "    recall = np.flipud(recall)\n",
    "    precision = np.flipud(precision)\n",
    "\n",
    "    mean_precision = np.interp(mean_recall, recall, precision)\n",
    "    all_precision.append(mean_precision)\n",
    "    AP.append(average_precision)\n",
    "\n",
    "# 输出结果\n",
    "print(round(statistics.mean(BACC_collecton),3),'±',round(statistics.stdev(BACC_collecton),3))\n",
    "print(round(statistics.mean(Sn_collecton),3),'±',round(statistics.stdev(Sn_collecton),3))\n",
    "print(round(statistics.mean(Sp_collecton),3),'±',round(statistics.stdev(Sp_collecton),3))\n",
    "print(round(statistics.mean(MCC_collecton),3),'±',round(statistics.stdev(MCC_collecton),3))\n",
    "print(round(statistics.mean(AUC_collecton),3),'±',round(statistics.stdev(AUC_collecton),3))\n",
    "print(round(statistics.mean(AP),3),'±',round(statistics.stdev(AP),3))\n",
    "\n",
    "# 在所有交叉验证循环结束后，计算TPR的均值\n",
    "mean_tpr = np.mean(interp_tpr_collection, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "\n",
    "# Calculate the mean precision\n",
    "mean_precision = np.mean(all_precision, axis=0)\n",
    "\n",
    "# 保存ROC曲线相关参数\n",
    "# np.savez(r'Draw graphics\\ROC curve\\PCA_All\\Capsule_Indenpendence.npz', fpr=base_fpr, tpr=mean_tpr, roc_auc=AUC_collecton)\n",
    "\n",
    "# 保存PR曲线相关参数\n",
    "# np.savez(r'Draw graphics\\PR curve\\PCA_All\\Capsule_Indenpendence.npz', recall=mean_recall, precision=mean_precision, average_precision=AP)\n",
    "\n",
    "# # 绘制ROC曲线\n",
    "# plt.figure()\n",
    "# lw = 2\n",
    "# plt.plot(base_fpr, mean_tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % np.mean(AUC_collecton))\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "# plt.xlim([-0.05, 1.05])\n",
    "# plt.ylim([-0.05, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('10 k-fold cross vaild')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "\n",
    "# 绘制PR曲线\n",
    "plt.figure(facecolor='white')\n",
    "lw=1\n",
    "\n",
    "# 设置字体\n",
    "font = {'family': 'Times New Roman', 'size': 12}\n",
    "plt.rc('font',family='Times New Roman')\n",
    "\n",
    "# 绘制曲线\n",
    "plt.step(mean_recall, mean_precision, color='#F97059', alpha=1, where='post', lw=lw, label='LR (AUPR=%0.4f)' % np.mean(AP))\n",
    "\n",
    "# 设置图表属性\n",
    "plt.xlabel('Recall', fontsize=14, weight='bold')\n",
    "plt.ylabel('Precision', fontsize=14, weight='bold')\n",
    "plt.title('Independence test - PR', fontsize=18, weight='bold')\n",
    "plt.legend(loc=\"lower left\", fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结尾"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

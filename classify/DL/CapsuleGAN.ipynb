{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current directory\n",
    "import os\n",
    "os.chdir('F:\\Work\\Experiment\\pLM4ACE\\model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_new = pd.read_csv(r\"fusion_features\\Data\\single\\AAI.csv\", index_col=0, header=None)\n",
    "y_new = pd.read_csv(\"fusion_features\\Data\\label.csv\", index_col=False, header=None)\n",
    "\n",
    "print(X_new.shape)\n",
    "print(y_new.shape)\n",
    "print(np.count_nonzero(y_new==0))\n",
    "print(np.count_nonzero(y_new==1))\n",
    "\n",
    "X_new = np.array(X_new, 'float32')\n",
    "y_new = np.array(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10折交叉验证\n",
    "import os,sys,math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import scale\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.activations import sigmoid\n",
    "from keras.layers import Input, Dense, Layer, Reshape, Flatten\n",
    "from keras.layers import multiply, Add, Permute\n",
    "from keras.layers import Dropout, Lambda, Concatenate, Multiply\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import adam_v2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_whole, X_ind_test, y_train_whole, y_ind_test = train_test_split(X_new, y_new, test_size=0.2, random_state=1111)\n",
    "\n",
    "# X_train_whole = scale(X_train_whole)\n",
    "[sample_num, input_dimwx]=np.shape(X_train_whole)\n",
    "X = X_train_whole\n",
    "y = y_train_whole\n",
    "\n",
    "def get_shuffle(data,label):    \n",
    "    #shuffle data\n",
    "    index = [i for i in range(len(label))]\n",
    "    np.random.shuffle(index)\n",
    "    data = data[index]\n",
    "    label = label[index]\n",
    "    return data,label \n",
    "\n",
    "(X, y) = get_shuffle(X, y)\n",
    "\n",
    "\n",
    "def scale_mean_var(input_arr,axis=0):\n",
    "    #from sklearn import preprocessing\n",
    "    #input_arr= preprocessing.scale(input_arr.astype('float'))\n",
    "    mean_ = np.mean(input_arr,axis=0)\n",
    "    scale_ = np.std(input_arr,axis=0)\n",
    "    #减均值 \n",
    "    output_arr= input_arr- mean_\n",
    "    #判断均值是否接近0\n",
    "    mean_1 = output_arr.mean(axis=0)\n",
    "    if not np.allclose(mean_1, 0):\n",
    "        output_arr -= mean_1\n",
    "    #将标准差为0元素的置1\n",
    "    #scale_ = _handle_zeros_in_scale(scale_, copy=False)\n",
    "    scale_[scale_ == 0.0] = 1.0\n",
    "    #除以标准差\n",
    "    output_arr /=scale_\n",
    "    #再次判断均值是否为0\n",
    "    mean_2 = output_arr .mean(axis=0)\n",
    "    if not np.allclose(mean_2, 0):\n",
    "        output_arr  -= mean_2\n",
    "\n",
    "    return output_arr\n",
    "\n",
    "\n",
    "########################################################### Def Cbam\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\tchannel = input_feature.shape[channel_axis]\n",
    "\tshared_layer_one = Dense(channel//ratio,\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t activation = 'relu',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\tshared_layer_two = Dense(channel,\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "\tavg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\tavg_pool = shared_layer_one(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tavg_pool = shared_layer_two(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\tmax_pool = GlobalMaxPooling2D()(input_feature)\n",
    "\tmax_pool = Reshape((1,1,channel))(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\tmax_pool = shared_layer_one(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tmax_pool = shared_layer_two(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\tcbam_feature = Add()([avg_pool,max_pool])\n",
    "\tcbam_feature = Activation('hard_sigmoid')(cbam_feature)\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "\tkernel_size = 7\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tchannel = input_feature.shape[1]\n",
    "\t\tcbam_feature = Permute((2,3,1))(input_feature)\n",
    "\telse:\n",
    "\t\tchannel = input_feature.shape[-1]\n",
    "\t\tcbam_feature = input_feature\n",
    "\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert avg_pool.shape[-1] == 1\n",
    "\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert max_pool.shape[-1] == 1\n",
    "\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "\tassert concat.shape[-1] == 2\n",
    "\tcbam_feature = Conv2D(filters = 1,\n",
    "\t\t\t\t\tkernel_size=kernel_size,\n",
    "\t\t\t\t\tactivation = 'hard_sigmoid',\n",
    "\t\t\t\t\tstrides=1,\n",
    "\t\t\t\t\tpadding='same',\n",
    "\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\tuse_bias=False)(concat)\n",
    "\tassert cbam_feature.shape[-1] == 1\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "def cbam_block(cbam_feature,ratio=8):\n",
    "\tcbam_feature = channel_attention(cbam_feature, ratio)\n",
    "\tcbam_feature = spatial_attention(cbam_feature, )\n",
    "\treturn cbam_feature\n",
    "\n",
    "\n",
    "############################################## Def discriminator and generator\n",
    "def squash(vectors, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "def build_discriminator():\n",
    "    img = Input(shape=(1,input_dimwx,1))\n",
    "    x = Conv2D(filters=64, kernel_size=(1,9), strides=2, padding='valid', name='conv1')(img)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(1,9), strides=2, padding='valid', name='conv1')(img)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    \"\"\"\n",
    "    NOTE: Capsule architecture starts from here.\n",
    "    \"\"\"\n",
    "    ##### primarycaps coming first ##### \n",
    "    x = Conv2D(filters=32, kernel_size=(1,3), strides=2, padding='valid', name='primarycap_conv2')(x)    \n",
    "    [aa,bb,cc,dd] = x.shape\n",
    "    numx = int(cc)\n",
    "    x = Reshape(target_shape=[-1, numx], name='primarycap_reshape')(x)\n",
    "    x = Lambda(squash, name='primarycap_squash')(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    ##### digitcaps are here ##### \n",
    "    x = Flatten()(x)\n",
    "    uhat = Dense(128, kernel_initializer='he_normal', bias_initializer='zeros', name='uhat_digitcaps')(x)\n",
    "    c = Activation('softmax', name='softmax_digitcaps1')(uhat) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(128)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    \"\"\"\n",
    "    NOTE: Squashing the capsule outputs creates severe blurry artifacts, thus we replace it with Leaky ReLu.\n",
    "    \"\"\"\n",
    "    s_j = LeakyReLU()(x)\n",
    "    ##### we will repeat the routing part 2 more times (num_routing=3) to unfold the loop\n",
    "    c = Activation('softmax', name='softmax_digitcaps2')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(128)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "\n",
    "    c = Activation('softmax', name='softmax_digitcaps3')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(128)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "    \n",
    "    c = Activation('softmax', name='softmax_digitcaps4')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(128)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "    # ##### preparition for cbam_block\n",
    "    s_j = Reshape((-1,128,1))(s_j)\n",
    "    inputs = s_j\n",
    "    residual = Conv2D(filters=64, kernel_size=(1,1), strides=1, padding='same', name='convxxx')(inputs)\n",
    "    residual = BatchNormalization(momentum=0.8)(residual)\n",
    "    \n",
    "    cbam = cbam_block(residual)\n",
    "    # cbam = channel_attention(residual)\n",
    "    # cbam = spatial_attention(residual)\n",
    "    \n",
    "    cbam = Reshape((-1,))(cbam)\n",
    "    pred = Dense(2, activation='sigmoid')(cbam)\n",
    "    \n",
    "    # cbam = Reshape((-1,))(s_j)\n",
    "    # pred = Dense(2, activation='sigmoid')(cbam)\n",
    "    \n",
    "    \n",
    "    return Model(img, pred)\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['binary_accuracy'])\n",
    "\n",
    "\n",
    "# generator structure\n",
    "def build_generator():\n",
    "    \"\"\"\n",
    "    Generator follows the DCGAN architecture and creates generated image representations through learning.\n",
    "    \"\"\"\n",
    "    noise_shape =(input_dimwx,)\n",
    "    x_noise = Input(shape=noise_shape)\n",
    "    # we apply different kernel sizes in order to match the original image size\n",
    "    x = Dense(64 * 1 * input_dimwx, activation=\"relu\")(x_noise)\n",
    "    x = Reshape((1, input_dimwx, 64))(x)\n",
    "    x = BatchNormalization(momentum=0.2)(x)\n",
    "    x = UpSampling2D()(x)\n",
    "    [aa1,bb1,cc1,dd1] = x.shape\n",
    "    numx1 = int(cc1//4)\n",
    "    x = Conv2D(32, kernel_size=(2,numx1), padding=\"valid\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(momentum=0.2)(x)\n",
    "    [aa2,bb2,cc2,dd2] = x.shape\n",
    "    #### x = UpSampling2D()(x)\n",
    "    numx2 = int(1+cc2-input_dimwx)\n",
    "    x = Conv2D(16, kernel_size=(1,numx2), padding=\"valid\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(momentum=0.2)(x)\n",
    "    x = Conv2D(1, kernel_size=3, padding=\"same\")(x)\n",
    "    gen_out = Activation(\"tanh\")(x)\n",
    "        \n",
    "    return Model(x_noise, gen_out)\n",
    "\n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam(0.002, 0.8), metrics=['binary_accuracy'])\n",
    "\n",
    "# feeding noise to generator\n",
    "z = Input(shape=(input_dimwx,))\n",
    "img = generator(z)\n",
    "# for the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "# try to discriminate generated images\n",
    "valid = discriminator(img)\n",
    "# the combined model (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity \n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam(0.002, 0.8), metrics=['binary_accuracy'])\n",
    "\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "AUC_collecton = []\n",
    "AP=[]\n",
    "\n",
    "mean_recall = np.linspace(0, 1, 100)\n",
    "all_precision = []\n",
    "base_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = 0.0\n",
    "# 新的TPR集合\n",
    "interp_tpr_collection = []\n",
    "\n",
    "\n",
    "def categorical_probas_to_classes(p):\n",
    "    return np.argmax(p, axis=1)\n",
    "\n",
    "\n",
    "def to_categorical(y, nb_classes=None):\n",
    "    y = np.array(y, dtype='int')\n",
    "    if not nb_classes:\n",
    "        nb_classes = np.max(y)+1\n",
    "    Y = np.zeros((len(y), nb_classes))\n",
    "    for i in range(len(y)):\n",
    "        Y[i, y[i]] = 1\n",
    "    return Y\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train, test in skf.split(X, y):\n",
    "    X_train, X_valid, y_train, y_valid = np.take(X, train.tolist(), axis=0), np.take(X, test.tolist(), axis=0), np.take(y, train.tolist(), axis=0), np.take(y, test.tolist(), axis=0)\n",
    "    y_train = to_categorical(y_train)\n",
    "    cv_clf = combined\n",
    "    hist = cv_clf.fit(X_train, y_train, batch_size=64, epochs=60)\n",
    "    y_score = cv_clf.predict(X_valid)\n",
    "    y_class = categorical_probas_to_classes(y_score)\n",
    "    TP, FP, FN, TN = confusion_matrix(y_valid, y_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
    "    Sn_collecton.append(TP/(TP+FN))\n",
    "    Sp_collecton.append(TN/(TN+FP))\n",
    "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "    MCC_collecton.append(MCC)\n",
    "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_valid, y_score[:, 1])\n",
    "    interp_tpr = np.interp(base_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    interp_tpr_collection.append(interp_tpr)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "    AUC_collecton.append(auc_roc)\n",
    "    # PR curve\n",
    "    precision, recall, _ = precision_recall_curve(y_valid, y_score[:, 1])\n",
    "    average_precision = average_precision_score(y_valid, y_score[:, 1])\n",
    "    recall = np.flipud(recall)\n",
    "    precision = np.flipud(precision)\n",
    "\n",
    "    mean_precision = np.interp(mean_recall, recall, precision)\n",
    "    all_precision.append(mean_precision)\n",
    "    AP.append(average_precision)\n",
    "\n",
    "# 输出结果\n",
    "print(round(statistics.mean(BACC_collecton),3),'±',round(statistics.stdev(BACC_collecton),3))\n",
    "print(round(statistics.mean(Sn_collecton),3),'±',round(statistics.stdev(Sn_collecton),3))\n",
    "print(round(statistics.mean(Sp_collecton),3),'±',round(statistics.stdev(Sp_collecton),3))\n",
    "print(round(statistics.mean(MCC_collecton),3),'±',round(statistics.stdev(MCC_collecton),3))\n",
    "print(round(statistics.mean(AUC_collecton),3),'±',round(statistics.stdev(AUC_collecton),3))\n",
    "print(round(statistics.mean(AP),3),'±',round(statistics.stdev(AP),3))\n",
    "\n",
    "# 在所有交叉验证循环结束后，计算TPR的均值\n",
    "mean_tpr = np.mean(interp_tpr_collection, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "\n",
    "# Calculate the mean precision\n",
    "mean_precision = np.mean(all_precision, axis=0)\n",
    "\n",
    "# 保存ROC曲线相关参数\n",
    "np.savez(r'Draw graphics\\ROC curve\\LR_cross_vaild.npz', fpr=base_fpr, tpr=mean_tpr, roc_auc=AUC_collecton)\n",
    "\n",
    "# 保存PR曲线相关参数\n",
    "np.savez(r'Draw graphics\\PR curve\\LR_Indenpendence.npz', recall=mean_recall, precision=mean_precision, average_precision=AP)\n",
    "\n",
    "# 绘制ROC曲线\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(base_fpr, mean_tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % np.mean(AUC_collecton))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('10 k-fold cross vaild')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 独立测试集\n",
    "import os,sys,math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import scale\n",
    "import tensorflow as tf\n",
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.activations import sigmoid\n",
    "from keras.layers import Input, Dense, Layer, Reshape, Flatten\n",
    "from keras.layers import multiply, Add, Permute\n",
    "from keras.layers import Dropout, Lambda, Concatenate, Multiply\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import adam_v2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# train_set = scale(X_new)\n",
    "[sample_num, input_dimwx]=np.shape(X_new)\n",
    "X = X_new\n",
    "y = y_new\n",
    "\n",
    "def get_shuffle(data,label):    \n",
    "    #shuffle data\n",
    "    index = [i for i in range(len(label))]\n",
    "    np.random.shuffle(index)\n",
    "    data = data[index]\n",
    "    label = label[index]\n",
    "    return data,label \n",
    "\n",
    "(X, y) = get_shuffle(X, y)\n",
    "\n",
    "\n",
    "def scale_mean_var(input_arr,axis=0):\n",
    "    #from sklearn import preprocessing\n",
    "    #input_arr= preprocessing.scale(input_arr.astype('float'))\n",
    "    mean_ = np.mean(input_arr,axis=0)\n",
    "    scale_ = np.std(input_arr,axis=0)\n",
    "    #减均值 \n",
    "    output_arr= input_arr- mean_\n",
    "    #判断均值是否接近0\n",
    "    mean_1 = output_arr.mean(axis=0)\n",
    "    if not np.allclose(mean_1, 0):\n",
    "        output_arr -= mean_1\n",
    "    #将标准差为0元素的置1\n",
    "    #scale_ = _handle_zeros_in_scale(scale_, copy=False)\n",
    "    scale_[scale_ == 0.0] = 1.0\n",
    "    #除以标准差\n",
    "    output_arr /=scale_\n",
    "    #再次判断均值是否为0\n",
    "    mean_2 = output_arr .mean(axis=0)\n",
    "    if not np.allclose(mean_2, 0):\n",
    "        output_arr  -= mean_2\n",
    "\n",
    "    return output_arr\n",
    "\n",
    "\n",
    "########################################################### Def Cbam\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\tchannel = input_feature.shape[channel_axis]\n",
    "\tshared_layer_one = Dense(channel//ratio,\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t activation = 'relu',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\tshared_layer_two = Dense(channel,\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\tavg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "\tavg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\tavg_pool = shared_layer_one(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tavg_pool = shared_layer_two(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\tmax_pool = GlobalMaxPooling2D()(input_feature)\n",
    "\tmax_pool = Reshape((1,1,channel))(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\tmax_pool = shared_layer_one(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tmax_pool = shared_layer_two(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\tcbam_feature = Add()([avg_pool,max_pool])\n",
    "\tcbam_feature = Activation('hard_sigmoid')(cbam_feature)\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "\tkernel_size = 7\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tchannel = input_feature.shape[1]\n",
    "\t\tcbam_feature = Permute((2,3,1))(input_feature)\n",
    "\telse:\n",
    "\t\tchannel = input_feature.shape[-1]\n",
    "\t\tcbam_feature = input_feature\n",
    "\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert avg_pool.shape[-1] == 1\n",
    "\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert max_pool.shape[-1] == 1\n",
    "\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "\tassert concat.shape[-1] == 2\n",
    "\tcbam_feature = Conv2D(filters = 1,\n",
    "\t\t\t\t\tkernel_size=kernel_size,\n",
    "\t\t\t\t\tactivation = 'hard_sigmoid',\n",
    "\t\t\t\t\tstrides=1,\n",
    "\t\t\t\t\tpadding='same',\n",
    "\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\tuse_bias=False)(concat)\n",
    "\tassert cbam_feature.shape[-1] == 1\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "def cbam_block(cbam_feature,ratio=8):\n",
    "\tcbam_feature = channel_attention(cbam_feature, ratio)\n",
    "\tcbam_feature = spatial_attention(cbam_feature, )\n",
    "\treturn cbam_feature\n",
    "\n",
    "\n",
    "############################################## Def discriminator and generator\n",
    "def squash(vectors, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "def build_discriminator():\n",
    "    img = Input(shape=(1,input_dimwx,1))\n",
    "    x = Conv2D(filters=64, kernel_size=(1,9), strides=2, padding='valid', name='conv1')(img)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=(1,9), strides=2, padding='valid', name='conv1')(img)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    \"\"\"\n",
    "    NOTE: Capsule architecture starts from here.\n",
    "    \"\"\"\n",
    "    ##### primarycaps coming first ##### \n",
    "    x = Conv2D(filters=32, kernel_size=(1,3), strides=2, padding='valid', name='primarycap_conv2')(x)    \n",
    "    [aa,bb,cc,dd] = x.shape\n",
    "    numx = int(cc)\n",
    "    x = Reshape(target_shape=[-1, numx], name='primarycap_reshape')(x)\n",
    "    x = Lambda(squash, name='primarycap_squash')(x)\n",
    "    x = BatchNormalization(momentum=0.8)(x)\n",
    "    \n",
    "    ##### digitcaps are here ##### \n",
    "    x = Flatten()(x)\n",
    "    uhat = Dense(128, kernel_initializer='he_normal', bias_initializer='zeros', name='uhat_digitcaps')(x)\n",
    "    c = Activation('softmax', name='softmax_digitcaps1')(uhat) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(128)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    \"\"\"\n",
    "    NOTE: Squashing the capsule outputs creates severe blurry artifacts, thus we replace it with Leaky ReLu.\n",
    "    \"\"\"\n",
    "    s_j = LeakyReLU()(x)\n",
    "    ##### we will repeat the routing part 2 more times (num_routing=3) to unfold the loop\n",
    "    c = Activation('softmax', name='softmax_digitcaps2')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(128)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "\n",
    "    c = Activation('softmax', name='softmax_digitcaps3')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(128)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "    \n",
    "    c = Activation('softmax', name='softmax_digitcaps4')(s_j) # softmax will make sure that each weight c_ij is a non-negative number and their sum equals to one\n",
    "    c = Dense(128)(c) # compute s_j\n",
    "    x = Multiply()([uhat, c])\n",
    "    s_j = LeakyReLU()(x)\n",
    "    # ##### preparition for cbam_block\n",
    "    s_j = Reshape((-1,128,1))(s_j)\n",
    "    inputs = s_j\n",
    "    residual = Conv2D(filters=64, kernel_size=(1,1), strides=1, padding='same', name='convxxx')(inputs)\n",
    "    residual = BatchNormalization(momentum=0.8)(residual)\n",
    "    \n",
    "    cbam = cbam_block(residual)\n",
    "    # cbam = channel_attention(residual)\n",
    "    # cbam = spatial_attention(residual)\n",
    "    \n",
    "    cbam = Reshape((-1,))(cbam)\n",
    "    pred = Dense(2, activation='sigmoid')(cbam)\n",
    "    \n",
    "    # cbam = Reshape((-1,))(s_j)\n",
    "    # pred = Dense(2, activation='sigmoid')(cbam)\n",
    "    \n",
    "    \n",
    "    return Model(img, pred)\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['binary_accuracy'])\n",
    "\n",
    "\n",
    "# generator structure\n",
    "def build_generator():\n",
    "    \"\"\"\n",
    "    Generator follows the DCGAN architecture and creates generated image representations through learning.\n",
    "    \"\"\"\n",
    "    noise_shape =(input_dimwx,)\n",
    "    x_noise = Input(shape=noise_shape)\n",
    "    # we apply different kernel sizes in order to match the original image size\n",
    "    x = Dense(64 * 1 * input_dimwx, activation=\"relu\")(x_noise)\n",
    "    x = Reshape((1, input_dimwx, 64))(x)\n",
    "    x = BatchNormalization(momentum=0.2)(x)\n",
    "    x = UpSampling2D()(x)\n",
    "    [aa1,bb1,cc1,dd1] = x.shape\n",
    "    numx1 = int(cc1//4)\n",
    "    x = Conv2D(32, kernel_size=(2,numx1), padding=\"valid\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(momentum=0.2)(x)\n",
    "    [aa2,bb2,cc2,dd2] = x.shape\n",
    "    #### x = UpSampling2D()(x)\n",
    "    numx2 = int(1+cc2-input_dimwx)\n",
    "    x = Conv2D(16, kernel_size=(1,numx2), padding=\"valid\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(momentum=0.2)(x)\n",
    "    x = Conv2D(1, kernel_size=3, padding=\"same\")(x)\n",
    "    gen_out = Activation(\"tanh\")(x)\n",
    "        \n",
    "    return Model(x_noise, gen_out)\n",
    "\n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam(0.002, 0.8), metrics=['binary_accuracy'])\n",
    "\n",
    "# feeding noise to generator\n",
    "z = Input(shape=(input_dimwx,))\n",
    "img = generator(z)\n",
    "# for the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "# try to discriminate generated images\n",
    "valid = discriminator(img)\n",
    "# the combined model (stacked generator and discriminator) takes\n",
    "# noise as input => generates images => determines validity \n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=adam_v2.Adam(0.002, 0.8), metrics=['binary_accuracy'])\n",
    "\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "AUC_collecton = []\n",
    "AP=[]\n",
    "mean_recall = np.linspace(0, 1, 100)\n",
    "all_precision = []\n",
    "\n",
    "base_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = 0.0\n",
    "# 新的TPR集合\n",
    "interp_tpr_collection = []\n",
    "\n",
    "\n",
    "def categorical_probas_to_classes(p):\n",
    "    return np.argmax(p, axis=1)\n",
    "\n",
    "\n",
    "def to_categorical(y, nb_classes=None):\n",
    "    y = np.array(y, dtype='int')\n",
    "    if not nb_classes:\n",
    "        nb_classes = np.max(y)+1\n",
    "    Y = np.zeros((len(y), nb_classes))\n",
    "    for i in range(len(y)):\n",
    "        Y[i, y[i]] = 1\n",
    "    return Y\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    # dataset splitting\n",
    "    X_train_whole, X_ind_test, y_train_whole, y_ind_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    y_train_whole = to_categorical(y_train_whole)\n",
    "    cv_clf = combined\n",
    "    hist = cv_clf.fit(X_train_whole, y_train_whole, batch_size=64, epochs=60)\n",
    "    y_score = cv_clf.predict(X_ind_test)\n",
    "    y_class = categorical_probas_to_classes(y_score)\n",
    "    TP, FP, FN, TN = confusion_matrix(y_ind_test, y_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
    "    Sn_collecton.append(TP/(TP+FN))\n",
    "    Sp_collecton.append(TN/(TN+FP))\n",
    "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "    MCC_collecton.append(MCC)\n",
    "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_ind_test, y_score[:, 1])\n",
    "    interp_tpr = np.interp(base_fpr, fpr, tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    interp_tpr_collection.append(interp_tpr)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "    AUC_collecton.append(auc_roc)\n",
    "    # PR curve\n",
    "    precision, recall, _ = precision_recall_curve(y_ind_test, y_score[:, 1])\n",
    "    average_precision = average_precision_score(y_ind_test, y_score[:, 1])\n",
    "    recall = np.flipud(recall)\n",
    "    precision = np.flipud(precision)\n",
    "\n",
    "    mean_precision = np.interp(mean_recall, recall, precision)\n",
    "    all_precision.append(mean_precision)\n",
    "    AP.append(average_precision)\n",
    "\n",
    "# 输出结果\n",
    "print(round(statistics.mean(BACC_collecton),3),'±',round(statistics.stdev(BACC_collecton),3))\n",
    "print(round(statistics.mean(Sn_collecton),3),'±',round(statistics.stdev(Sn_collecton),3))\n",
    "print(round(statistics.mean(Sp_collecton),3),'±',round(statistics.stdev(Sp_collecton),3))\n",
    "print(round(statistics.mean(MCC_collecton),3),'±',round(statistics.stdev(MCC_collecton),3))\n",
    "print(round(statistics.mean(AUC_collecton),3),'±',round(statistics.stdev(AUC_collecton),3))\n",
    "print(round(statistics.mean(AP),3),'±',round(statistics.stdev(AP),3))\n",
    "\n",
    "# 在所有交叉验证循环结束后，计算TPR的均值\n",
    "mean_tpr = np.mean(interp_tpr_collection, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "\n",
    "# Calculate the mean precision\n",
    "mean_precision = np.mean(all_precision, axis=0)\n",
    "\n",
    "# 保存ROC曲线相关参数\n",
    "np.savez(r'Draw graphics\\ROC curve\\LR_cross_vaild.npz', fpr=base_fpr, tpr=mean_tpr, roc_auc=AUC_collecton)\n",
    "\n",
    "# 保存PR曲线相关参数\n",
    "np.savez(r'Draw graphics\\PR curve\\LR_Indenpendence.npz', recall=mean_recall, precision=mean_precision, average_precision=AP)\n",
    "\n",
    "# 绘制ROC曲线\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(base_fpr, mean_tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % np.mean(AUC_collecton))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('10 k-fold cross vaild')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结尾"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
